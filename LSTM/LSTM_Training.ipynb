{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN97mO5lgsIhb7QPMAZVKhW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRRTnPeYqzNG","executionInfo":{"status":"ok","timestamp":1744610338446,"user_tz":240,"elapsed":10096,"user":{"displayName":"Harman B","userId":"00357522275080643694"}},"outputId":"1f73a5c6-8673-4085-e02f-1243f7d58719"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/386.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/231.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/231.9 kB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install optuna --quiet"]},{"cell_type":"code","source":["import time\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","from torch.optim import AdamW\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score\n","import optuna\n","import matplotlib.pyplot as plt\n","import os\n","import pickle"],"metadata":{"id":"W97GG1qSq2nx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using:\",device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eKbXKMBrDVt","executionInfo":{"status":"ok","timestamp":1744610474711,"user_tz":240,"elapsed":103752,"user":{"displayName":"Harman B","userId":"00357522275080643694"}},"outputId":"0527d69f-4f24-4bd7-c5c9-cb43a361e4c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Using: cuda\n"]}]},{"cell_type":"code","source":["# Path to the database file\n","db_path = \"/content/drive/My Drive/ADL Final Project/optuna_study.db\"\n","\n","# Create or load the study\n","study = optuna.create_study(\n","    study_name = \"optuna_study\",\n","    storage = f\"sqlite:///{db_path}\",\n","    load_if_exists = True,\n","    direction = \"maximize\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnissLuprFdF","executionInfo":{"status":"ok","timestamp":1744610477499,"user_tz":240,"elapsed":2794,"user":{"displayName":"Harman B","userId":"00357522275080643694"}},"outputId":"4ab83175-0379-4321-fd4c-dbef58fb35ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-04-14 06:01:17,549] A new study created in RDB with name: optuna_study\n"]}]},{"cell_type":"code","source":["best_params = {'lstm_units': 64, 'learning_rate': 0.009885647542627366, 'weight_decay': 0.00959695196216962}\n","print(best_params)\n","best_lstm_units = best_params['lstm_units']\n","best_learning_rate = best_params['learning_rate']\n","best_weight_decay = best_params['weight_decay']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUUVebLerZtV","executionInfo":{"status":"ok","timestamp":1744610477502,"user_tz":240,"elapsed":32,"user":{"displayName":"Harman B","userId":"00357522275080643694"}},"outputId":"14fb3cd7-c6cb-461d-b1eb-3fe8cd91d49b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'lstm_units': 64, 'learning_rate': 0.009885647542627366, 'weight_decay': 0.00959695196216962}\n"]}]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/My Drive/ADL Final Project/yelp_dataset_train.csv\")\n","test_df = pd.read_csv(\"/content/drive/My Drive/ADL Final Project/yelp_dataset_test.csv\")\n","\n","texts = train_df['review_text'].tolist()\n","labels = train_df['class_index'].tolist()\n","\n","vocab_size = 20000\n","tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n","tokenizer.fit_on_texts(texts)\n","\n","train_sequences = tokenizer.texts_to_sequences(train_df['review_text'])\n","test_sequences = tokenizer.texts_to_sequences(test_df['review_text'])\n","\n","max_length = 512\n","\n","train_padded = pad_sequences(train_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n","test_padded = pad_sequences(test_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_padded, labels, test_size = 0.2, stratify = labels, random_state = 42)"],"metadata":{"id":"kd5rdpEVr7OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert padded sequences and labels to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype = torch.long)\n","y_train_tensor = torch.tensor(y_train, dtype = torch.long)\n","X_val_tensor = torch.tensor(X_val, dtype = torch.long)\n","y_val_tensor = torch.tensor(y_val, dtype = torch.long)\n","X_test_tensor = torch.tensor(test_padded, dtype = torch.long)\n","\n","# Create Dataset objects\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","test_dataset = TensorDataset(X_test_tensor, torch.tensor(test_df['class_index'].values, dtype = torch.long))\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)\n","val_loader = DataLoader(val_dataset, batch_size = 128, shuffle = False)\n","test_loader = DataLoader(test_dataset, batch_size = 128, shuffle = False)"],"metadata":{"id":"h1KZD-HjxU2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BiLSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout):\n","        super(BiLSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, bidirectional = True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        pooled = torch.mean(lstm_out, dim = 1)\n","        out = self.dropout(pooled)\n","        out = self.fc(out)\n","        return self.softmax(out)"],"metadata":{"id":"MUvAEh-VyNmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BiLSTMClassifier(vocab_size = 20000, embedding_dim = 100, hidden_dim = best_lstm_units, output_dim = 3,  dropout = 0.3).to(device)"],"metadata":{"id":"jXy_9F9wykJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr = best_learning_rate, weight_decay = best_weight_decay)"],"metadata":{"id":"41PcLHNhy-Yf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","\n","# Track metrics\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","val_f1s = []\n","val_recalls = []\n","val_precisions = []\n","epoch_times = []\n","\n","total_start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","\n","    # ---------- Training ----------\n","    model.train()\n","    epoch_train_losses = []\n","\n","    for batch in train_loader:\n","        inputs, targets = batch\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_train_losses.append(loss.item())\n","\n","    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n","    train_losses.append(avg_train_loss)\n","\n","    # ---------- Validation ----------\n","    model.eval()\n","    epoch_val_losses = []\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            inputs, targets = batch\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            epoch_val_losses.append(loss.item())\n","\n","            preds = torch.argmax(outputs, dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_targets.extend(targets.cpu().numpy())\n","\n","    avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses)\n","    val_accuracy = accuracy_score(all_targets, all_preds)\n","    val_f1 = f1_score(all_targets, all_preds, average=\"weighted\")\n","    val_recall = recall_score(all_targets, all_preds, average=\"weighted\")\n","    val_precision = precision_score(all_targets, all_preds, average=\"weighted\")\n","\n","    val_losses.append(avg_val_loss)\n","    val_accuracies.append(val_accuracy)\n","    val_f1s.append(val_f1)\n","    val_recalls.append(val_recall)\n","    val_precisions.append(val_precision)\n","\n","    epoch_time = time.time() - epoch_start_time\n","    epoch_times.append(epoch_time)\n","\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f} sec\")\n","    print(f\"Train Loss      : {avg_train_loss:.4f}\")\n","    print(f\"Val Loss        : {avg_val_loss:.4f}\")\n","    print(f\"Val Accuracy    : {val_accuracy:.4f}\")\n","    print(f\"Val F1 Score    : {val_f1:.4f}\")\n","    print(f\"Val Recall      : {val_recall:.4f}\")\n","    print(f\"Val Precision   : {val_precision:.4f}\")\n","\n","total_training_time = time.time() - total_start_time\n","print(f\"\\nTotal Training Time: {total_training_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17kavSUBzPNG","executionInfo":{"status":"ok","timestamp":1744611134446,"user_tz":240,"elapsed":573224,"user":{"displayName":"Harman B","userId":"00357522275080643694"}},"outputId":"8b70c978-3013-4463-beac-d095f1ff2545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/10 - Time: 53.76 sec\n","Train Loss      : 0.7534\n","Val Loss        : 0.7138\n","Val Accuracy    : 0.8341\n","Val F1 Score    : 0.8344\n","Val Recall      : 0.8341\n","Val Precision   : 0.8365\n","\n","Epoch 2/10 - Time: 56.36 sec\n","Train Loss      : 0.7066\n","Val Loss        : 0.7178\n","Val Accuracy    : 0.8319\n","Val F1 Score    : 0.8313\n","Val Recall      : 0.8319\n","Val Precision   : 0.8313\n","\n","Epoch 3/10 - Time: 57.61 sec\n","Train Loss      : 0.7011\n","Val Loss        : 0.7040\n","Val Accuracy    : 0.8449\n","Val F1 Score    : 0.8451\n","Val Recall      : 0.8449\n","Val Precision   : 0.8458\n","\n","Epoch 4/10 - Time: 58.11 sec\n","Train Loss      : 0.6951\n","Val Loss        : 0.6988\n","Val Accuracy    : 0.8487\n","Val F1 Score    : 0.8477\n","Val Recall      : 0.8487\n","Val Precision   : 0.8478\n","\n","Epoch 5/10 - Time: 57.62 sec\n","Train Loss      : 0.6901\n","Val Loss        : 0.6957\n","Val Accuracy    : 0.8519\n","Val F1 Score    : 0.8523\n","Val Recall      : 0.8519\n","Val Precision   : 0.8534\n","\n","Epoch 6/10 - Time: 58.30 sec\n","Train Loss      : 0.6848\n","Val Loss        : 0.6944\n","Val Accuracy    : 0.8540\n","Val F1 Score    : 0.8537\n","Val Recall      : 0.8540\n","Val Precision   : 0.8538\n","\n","Epoch 7/10 - Time: 58.42 sec\n","Train Loss      : 0.6835\n","Val Loss        : 0.6898\n","Val Accuracy    : 0.8581\n","Val F1 Score    : 0.8579\n","Val Recall      : 0.8581\n","Val Precision   : 0.8579\n","\n","Epoch 8/10 - Time: 57.60 sec\n","Train Loss      : 0.6818\n","Val Loss        : 0.6935\n","Val Accuracy    : 0.8549\n","Val F1 Score    : 0.8554\n","Val Recall      : 0.8549\n","Val Precision   : 0.8563\n","\n","Epoch 9/10 - Time: 57.82 sec\n","Train Loss      : 0.6827\n","Val Loss        : 0.6929\n","Val Accuracy    : 0.8555\n","Val F1 Score    : 0.8544\n","Val Recall      : 0.8555\n","Val Precision   : 0.8548\n","\n","Epoch 10/10 - Time: 57.58 sec\n","Train Loss      : 0.6823\n","Val Loss        : 0.6955\n","Val Accuracy    : 0.8530\n","Val F1 Score    : 0.8514\n","Val Recall      : 0.8530\n","Val Precision   : 0.8525\n","\n","Total Training Time: 573.18 seconds\n"]}]},{"cell_type":"code","source":["final_model_dir = \"/content/drive/My Drive/ADL Final Project/lstm_final_model\"\n","os.makedirs(final_model_dir, exist_ok=True)\n","\n","# Save the model state_dict\n","model_path = os.path.join(final_model_dir, \"bilstm_model.pth\")\n","torch.save(model.state_dict(), model_path)"],"metadata":{"id":"M9JPBNGN0M8i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer_path = \"/content/drive/My Drive/ADL Final Project/lstm_final_model/tokenizer.pkl\"\n","\n","# Save tokenizer\n","with open(tokenizer_path, 'wb') as f:\n","    pickle.dump(tokenizer, f)"],"metadata":{"id":"TExfgneU9k7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7N93Y7E_9tCw"},"execution_count":null,"outputs":[]}]}